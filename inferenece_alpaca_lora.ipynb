{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/attianopp/llm-testing/blob/main/inferenece_alpaca_lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ExAe1UCWn5E"
      },
      "source": [
        "# Talk to Alpaca-LoRA\n",
        "\n",
        "This notebook contains minimal code for running [Alpaca-LoRA](https://github.com/tloen/alpaca-lora/) for demonstration purposes. Please check the repo for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_pz8MuY84Qh",
        "outputId": "2608819f-315c-46eb-cd88-a883ca899e0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.9/dist-packages (0.37.2)\n",
            "\u001b[33m  WARNING: Did not find branch or tag 'c3dc391', assuming revision or ref.\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes\n",
        "!pip install -q datasets loralib sentencepiece\n",
        "!pip install -q git+https://github.com/zphang/transformers@c3dc391\n",
        "!pip install -q git+https://github.com/huggingface/peft.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VucO3HSMoJkz"
      },
      "outputs": [],
      "source": [
        "# from peft import PeftModel\n",
        "# from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "# tokenizer = LLaMATokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")\n",
        "# model = LLaMAForCausalLM.from_pretrained(\n",
        "#     \"decapoda-research/llama-7b-hf\",\n",
        "#     load_in_8bit=True,\n",
        "#     device_map=\"auto\",\n",
        "# )\n",
        "# model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 775,
          "referenced_widgets": [
            "a5e7f15faf3a41e2a56644b5740c2d3e",
            "fbcf335da19b4775b66463f279d42b6a",
            "4bbcfd16aeaf4b3489500db13f580769",
            "1ed3e89e81b54329b24e4f61c1511be1",
            "4907efd57b764902aa74b70623be072e",
            "4ee865e2ed3f4835851b15e756a59411",
            "53c001d8fc1049be945d22606266988f",
            "a3757ab1964f494a907fb5082649ca50",
            "89a1e6bb47704d5faa58f6b2189bbcc0",
            "944ed82b71554f7483121c9f5aa474bb",
            "84c372f35885450395b3a8534246000b",
            "acbe957d71c54e8182c0df52f9c4b332",
            "02e0430ac4fe48c483630c8813f4b40a",
            "be5a1ef8d1f2409a8887c67be5e2c1ad",
            "7c1efe4cb5f7409ead8adc9fd5b42998",
            "6f4671132b52401fa5887b8ca26a54be",
            "57242f70eb84406ca674dae879b39b68",
            "2886b87d5c964b588e0337d6bd02b892",
            "2a0939269ca94bab9fd2e575e35af9d1",
            "852bd89f5fc04f5d89da730b9b30b576",
            "749e5ff5c17a478396ec7b2a69ecb919",
            "ec6f1e69e60f49b88f3d6f2ca11a0878",
            "0f087c0582494091a6e8a96c92063f0e",
            "16b90d50ecff4c539d008aa611d477fc",
            "74389e23639447b086544d90a3bb9df0",
            "a1153cbbb40e4078837e7de3e9cc71f5",
            "a809cdc010374bf28b5a1e50d9d5608f",
            "b47a3ce840c74c89ad7316f7fae98592",
            "5e193bbe28de451bab20e3350e270863",
            "23a5f86daa59401d88ced40aef197165",
            "412413fb91484215896ab3ccecd3a7be",
            "c84df414b9b14052bd6a11bda00ee602",
            "6b85387acca94c018d91398b26fba5a5",
            "34e29c0c334e4afc86db23a83e1bdd2a",
            "32c25a3200834682a806faceb5d1ec85",
            "8ec0d55ec48a4893b200d328b0475e07",
            "d334b4964bee49d0a04aeb1e0fe52b9b",
            "72d86cff7592422dba783107b28a0407",
            "7802be91c83f42fab286858513cfaff1",
            "447a236251ee41aea0327711b0c3a759",
            "ae960d8ecb7c4263a66f9e8691a05ca1",
            "aa3a135ae40f457bac7c851e11ab0c2f"
          ]
        },
        "id": "UceKDg3FMvJL",
        "outputId": "d5df9063-be22-4338-8f8a-b18a4ed94e84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching /usr/local/cuda/lib64...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.9/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /usr/lib64-nvidia did not contain libcudart.so as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-gq2hcnj7gwtr --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.9/dist-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5e7f15faf3a41e2a56644b5740c2d3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbcf335da19b4775b66463f279d42b6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4bbcfd16aeaf4b3489500db13f580769",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ed3e89e81b54329b24e4f61c1511be1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/427 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4907efd57b764902aa74b70623be072e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)model.bin.index.json:   0%|          | 0.00/25.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ee865e2ed3f4835851b15e756a59411",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00001-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "53c001d8fc1049be945d22606266988f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00002-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3757ab1964f494a907fb5082649ca50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00003-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "89a1e6bb47704d5faa58f6b2189bbcc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00004-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "944ed82b71554f7483121c9f5aa474bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00005-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84c372f35885450395b3a8534246000b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00006-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "acbe957d71c54e8182c0df52f9c4b332",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00007-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02e0430ac4fe48c483630c8813f4b40a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00008-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be5a1ef8d1f2409a8887c67be5e2c1ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00009-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c1efe4cb5f7409ead8adc9fd5b42998",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00010-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f4671132b52401fa5887b8ca26a54be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00011-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57242f70eb84406ca674dae879b39b68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00012-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2886b87d5c964b588e0337d6bd02b892",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00013-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a0939269ca94bab9fd2e575e35af9d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00014-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "852bd89f5fc04f5d89da730b9b30b576",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00015-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "749e5ff5c17a478396ec7b2a69ecb919",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00016-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec6f1e69e60f49b88f3d6f2ca11a0878",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00017-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f087c0582494091a6e8a96c92063f0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00018-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16b90d50ecff4c539d008aa611d477fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00019-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74389e23639447b086544d90a3bb9df0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00020-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1153cbbb40e4078837e7de3e9cc71f5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00021-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a809cdc010374bf28b5a1e50d9d5608f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00022-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b47a3ce840c74c89ad7316f7fae98592",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00023-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e193bbe28de451bab20e3350e270863",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00024-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23a5f86daa59401d88ced40aef197165",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00025-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "412413fb91484215896ab3ccecd3a7be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00026-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c84df414b9b14052bd6a11bda00ee602",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00027-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b85387acca94c018d91398b26fba5a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00028-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34e29c0c334e4afc86db23a83e1bdd2a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00029-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32c25a3200834682a806faceb5d1ec85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00030-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ec0d55ec48a4893b200d328b0475e07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00031-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d334b4964bee49d0a04aeb1e0fe52b9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00032-of-00033.bin:   0%|          | 0.00/405M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72d86cff7592422dba783107b28a0407",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)l-00033-of-00033.bin:   0%|          | 0.00/524M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7802be91c83f42fab286858513cfaff1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "447a236251ee41aea0327711b0c3a759",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae960d8ecb7c4263a66f9e8691a05ca1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/adapter_config.json:   0%|          | 0.00/399 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa3a135ae40f457bac7c851e11ab0c2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading adapter_model.bin:   0%|          | 0.00/67.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LLaMAForCausalLM(\n",
              "      (model): LLaMAModel(\n",
              "        (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
              "        (layers): ModuleList(\n",
              "          (0): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (1): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (2): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (3): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (4): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (5): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (6): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (7): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (8): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (9): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (10): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (11): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (12): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (13): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (14): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (15): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (16): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (17): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (18): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (19): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (20): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (21): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (22): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (23): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (24): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (25): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (26): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (27): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (28): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (29): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (30): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "          (31): LLaMADecoderLayer(\n",
              "            (self_attn): LLaMAAttention(\n",
              "              (q_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (k_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (v_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (o_proj): Linear8bitLt(\n",
              "                in_features=4096, out_features=4096, bias=False\n",
              "                (lora_dropout): Dropout(p=0.05, inplace=False)\n",
              "                (lora_A): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                (lora_B): Linear(in_features=16, out_features=4096, bias=False)\n",
              "              )\n",
              "              (rotary_emb): RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LLaMAMLP(\n",
              "              (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
              "              (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): RMSNorm()\n",
              "            (post_attention_layernorm): RMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): RMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig\n",
        "\n",
        "# LOAD PRE TRAINED\n",
        "peft_model_id = \"/content/\"\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "\n",
        "tokenizer = LLaMATokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "model = LLaMAForCausalLM.from_pretrained(\n",
        "    config.base_model_name_or_path,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, \"tloen/alpaca-lora-7b\")\n",
        "\n",
        "model = model.cuda()\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4NjNvMh2TuEm"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w3_lzwcqermJ"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{input}\n",
        "\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Response:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Egh3beCVRpW5"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    num_beams=4,\n",
        ")\n",
        "\n",
        "def evaluate(instruction, input=None):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].cuda()\n",
        "    generation_output = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        generation_config=generation_config,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        max_new_tokens=1000\n",
        "    )\n",
        "    for s in generation_output.sequences:\n",
        "        output = tokenizer.decode(s)\n",
        "        print(\"Response:\", output.split(\"### Response:\")[1].strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upOB2AQJSW9-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "evaluate(input(\"Instruction: \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW19-3qHgrTj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "evaluate(input(\"Instruction: \"))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}